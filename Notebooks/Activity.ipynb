{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee33b6bb-6674-484e-83a0-dad482d03f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5370dad-f83c-4ee3-ac1d-f79d0782882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:8080/v1\", # \"http://<Your api-server IP>:port\"\n",
    "    api_key = \"sk-no-key-required\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb63309c-5c28-4a6d-8b76-7b4fcac81e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A LLM, or a Master of Laws, is a graduate degree in law that is typically awarded after completing a law degree. It is a professional degree that requires a significant amount of study and research, and it is often sought after by lawyers and legal professionals who want to specialize in a particular area of law or jurisdiction.</s>\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"TinyLLM\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a LLM?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef0cc863-1766-4e11-8ffc-f15d57b2fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of artificial intelligence (AI) model that can generate human-like language. It is a type of neural network that can process large amounts of text data and generate natural-sounding sentences. LLMs are trained on large datasets of text, such as Wikipedia, and are capable of generating human-like language with varying degrees of accuracy. They are used in a variety of applications, including natural language processing (NLP), machine translation, and chatbots.</s>\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"TinyLLM\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a Large Language Model?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5a7acbf-32b1-47a9-821f-864c478afcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a machine learning algorithm designed to process large volumes of text in a language. It is commonly used in natural language processing (NLP) tasks, such as text generation, translation, and summarization. LLMs use a vast corpus of text, often billions or trillions of words, to learn how to generate or translate text based on the context of the input. They are often trained on a combination of unstructured and structured data, such as news articles, web pages, and social media posts. In summary, LLMs are large and powerful models that can process large volumes of text in a specific language.</s>\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"TinyLLM\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a Large Language Model?\"}\n",
    "  ],\n",
    "    temperature=0.70\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "045ae5b8-eee4-4102-9648-90747a80cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of artificial intelligence (AI) model that can process enormous amounts of text data. LLMs are designed to generate human-like text by learning from large amounts of text data, including blog posts, social media posts, news articles, and more. They typically use deep learning algorithms to analyze the text and generate text based on the context of the original input. LLMs are used in a variety of applications, including chatbots, natural language processing (NLP), and machine translation. They have the potential to revolutionize many aspects of human communication and knowledge acquisition.</s>\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"TinyLLM\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a Large Language Model?\"}\n",
    "  ],\n",
    "    temperature=0.70\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50ff7cfe-2c5c-4bdd-8605-4150c3c11c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of artificial intelligence model that can process large amounts of text data. It is often used for natural language processing tasks such as text classification, sentiment analysis, and language translation. LLMs are trained on large datasets of text, which can be in the form of textual data, such as news articles, social media posts, or academic papers. The model is trained to recognize patterns and relationships in the text data, which can help it to make more accurate predictions or classifications. LLMs are often used in industries such as finance, healthcare, and legal services, where they can help to improve the accuracy and efficiency of their respective processes.</s>\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"TinyLLM\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a Large Language Model?\"}\n",
    "  ],\n",
    "    temperature=0.40\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4796a973-31b7-439e-bf7c-e21865cae95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of machine learning model used for language translation, text generation, and natural language processing tasks. LLMs are trained on massive amounts of data, including text data from various languages, to improve their ability to translate text from one language to another. LLMs use complex algorithms to analyze and understand the structure and syntax of the source language and the target language, allowing them to generate accurate translations in real-time. Unlike traditional machine translation engines, LLMs can generate high-quality translations quickly and accurately, making them an essential tool for language translation applications.</s>\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"TinyLLM\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a Large Language Model?\"}\n",
    "  ],\n",
    "    temperature=0.90\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9364d53e-db27-4a2c-82f8-eefa10c56a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of artificial intelligence model that can process vast amounts of text data. It is made up of a large number of language models, each of which is trained on a specific corpus of text.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"TinyLLM\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a Large Language Model?\"}\n",
    "  ],\n",
    "    temperature=0.70,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9402da0e-905b-4925-bf38-bc8e5e569537",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = client.embeddings.create(\n",
    "    model='TinyLLM',\n",
    "    input=\"What is a LLM?\"\n",
    ")\n",
    "\n",
    "q2 = client.embeddings.create(\n",
    "    model='TinyLLM',\n",
    "    input=\"What is a Large Language Model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29121eca-e5b7-45f2-a7fb-d4678c4bd9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.005105342250317335,\n",
       " -0.021652555093169212,\n",
       " -0.01145350094884634,\n",
       " 0.0023372278083115816,\n",
       " -0.02683500200510025,\n",
       " -0.02535470761358738,\n",
       " 0.0010735621908679605,\n",
       " -0.010722712613642216,\n",
       " -0.013440721668303013,\n",
       " -0.012307122349739075,\n",
       " 0.019473671913146973,\n",
       " 0.0029807817190885544,\n",
       " -0.017625145614147186,\n",
       " -0.0037848397623747587,\n",
       " 0.0004427048552315682,\n",
       " -0.012478766962885857,\n",
       " 0.013820169493556023,\n",
       " 0.0006598757463507354,\n",
       " -0.031564369797706604,\n",
       " 0.02937830053269863]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.data[0].embedding[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9198866-7ac6-4fb6-8836-8629e8c6f943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.010369114577770233,\n",
       " -0.013365156948566437,\n",
       " -0.0027918058913201094,\n",
       " -0.012412614189088345,\n",
       " -0.015157658606767654,\n",
       " -0.01728396862745285,\n",
       " -0.0013681371929123998,\n",
       " -0.019303595647215843,\n",
       " -0.008755148388445377,\n",
       " -0.02654951438307762,\n",
       " 0.02182239480316639,\n",
       " 0.00059725739993155,\n",
       " 0.009518014267086983,\n",
       " 0.022850697860121727,\n",
       " 0.0008401128579862416,\n",
       " -0.011973206885159016,\n",
       " 0.02467368170619011,\n",
       " 0.006160115357488394,\n",
       " -0.04989228397607803,\n",
       " 0.01488324161618948]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2.data[0].embedding[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8c5f914-11cc-4b08-96fe-e6209c0b7415",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"TinyLLM\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.Respond in markdown\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write an example of a Hello World in Python with explanation.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e888e34-9519-402d-80e9-e8026687a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "# This is a Hello World program in Python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "In this program, we have created a simple program that prints the string \"Hello, World!\" to the console. The `print()` function is used to output the string to the console.</s>\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79e26cca-e88c-41c2-8f1f-b439c00e632a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "# This is a Hello World program in Python\n",
       "print(\"Hello, World!\")\n",
       "```\n",
       "\n",
       "In this program, we have created a simple program that prints the string \"Hello, World!\" to the console. The `print()` function is used to output the string to the console.</s>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8401ea8-5368-4134-8771-a0b59bcd36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def responses(user_prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"TinyLLM\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.Respond in markdown\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19115c32-8da3-48d3-bb1f-92f340e8d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = responses(\"Write me a Hello World example in Java with explanations and steps in bullet points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2be5bea9-824c-421d-abe7-7ac8a394d1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, here's a Hello World example in Java with explanations and steps:\n",
       "\n",
       "```java\n",
       "public class HelloWorld {\n",
       "    public static void main(String[] args) {\n",
       "        System.out.println(\"Hello, World!\");\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "1. Create a new Java project in your IDE (e.g. IntelliJ IDEA)\n",
       "2. Open the `main` class in the `HelloWorld` package\n",
       "3. Add the following code:\n",
       "   ```java\n",
       "   public class HelloWorld {\n",
       "       public static void main(String[] args) {\n",
       "           System.out.println(\"Hello, World!\");\n",
       "       }\n",
       "   }\n",
       "   ```\n",
       "4. Save the file and compile it using the `javac` command:\n",
       "   ```\n",
       "   javac -cp .:lib/junit-4.12.jar HelloWorld.java\n",
       "   ```\n",
       "5. Run the program using the `java` command:\n",
       "   ```\n",
       "   java -cp .:lib/junit-4.12.jar HelloWorld\n",
       "   ```\n",
       "6. You should see the message \"Hello, World!\" printed to the console.\n",
       "\n",
       "I hope this helps! Let me know if you have any further questions.</s>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4797a892-90ae-4fbd-8a1f-37854d393d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.embeddings.create(\n",
    "    model='TinyLLM',\n",
    "    input=\"The cat in the hat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45789bcd-a790-4274-ba7f-1132a6c2047b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17184bf4-2757-4fa9-98a9-6e24786fa882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-openai pypdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d65925b9-9042-4779-afcf-6a575ef69706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92f43cf8-801c-4f85-9ff8-b45174b412ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'https://arxiv.org/pdf/2012.12931.pdf'\n",
    "pdf_loader = PyPDFLoader(file_path = pdf_path)\n",
    "pdf_data = pdf_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3b8f624-bfbd-4898-ae72-40435e38290a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29d3c464-7668-4dcc-8b15-9842f618a9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Using Classification Datasets to Evaluate Graph Outlier\n",
      "Detection: Peculiar Observations and New Insights\n",
      "LINGXIAO ZHAO, Heinz College - Information Systems & Public Policy, Carnegie Mellon University, USA\n",
      "LEMAN AKOGLU, Heinz College - Information Systems & Public Policy, Carnegie Mellon University, USA\n",
      "It is common practice of the outlier mining community to repurpose classification datasets toward evaluating\n",
      "various detection models. To that end, often a binary classification dataset is used, where samples from\n",
      "one of the classes is designated as the ‚Äòinlier‚Äô samples, and the other class is substantially down-sampled\n",
      "to create the (ground-truth) ‚Äòoutlier‚Äô samples. Graph-level outlier detection (GLOD) is rarely studied but\n",
      "has many potentially influential real-world applications. In this study, we identify an intriguing issue with\n",
      "repurposing graph classification datasets for GLOD . We find that ROC-AUC performance of the models\n",
      "changes significantly (‚Äúflips‚Äù from high to very low, even worse than random) depending on which class is\n",
      "down-sampled. Interestingly, ROC-AUCs on these two variants approximately sum to 1 and their performance\n",
      "gap is amplified with increasing propagations for a certain family of propagation based outlier detection\n",
      "models. We carefully study the graph embedding space produced by propagation based models and find\n",
      "two driving factors: (1) disparity between within-class densities which is amplified by propagation, and (2)\n",
      "overlapping support (mixing of embeddings) across classes. We also study other graph embedding methods and\n",
      "downstream outlier detectors, and find that the intriguing ‚Äúperformance flip‚Äù issue still widely exists but which\n",
      "version of the downsample achieves higher performance may vary. Thoughtful analysis over comprehensive\n",
      "results further deeper our understanding of the established issue.\n",
      "With this study, we aim to draw attention to this (to our knowledge) previously-unnoticed issue for the\n",
      "rarely studied GLOD problem, and specifically to the following questions: 1) Given the performance flip\n",
      "issue we identified, where one version of the downsample often yields worse-than-random performance, is it\n",
      "acppropriate to evaluate GLOD by average performance across all downsampled versions when repurposing\n",
      "graph classification datasets? 2) Considering onsistently observed performance flip issue across different graph\n",
      "embedding methods we studied, is it possible to design better graph embedding methods to to overcome the\n",
      "issue? We conclude the paper with our insights to these questions.\n",
      "1 INTRODUCTION\n",
      "Outlier detection is a critical task that finds numerous applications in healthcare, security, finance,\n",
      "etc. [ 1]. Simply put, the task is to identify observations that notably stand out within large collections\n",
      "of data so as to ‚Äúarouse suspicions that [they were] generated by a different mechanism‚Äù [ 12]. One\n",
      "of the key challenges of outlier detection is that it poses an unsupervised learning problem. Due\n",
      "to the rare nature of outlier instances, combined with the laborious manual (i.e., human) labeling,\n",
      "access to benchmark datasets with sufficiently many labeled ground-truth outliers is limited.\n",
      "Motivation. Lack of labeled benchmark datasets for outlier detection is not only a challenge\n",
      "for learning, but also for the evaluation of outlier models. Even if one designs unsupervised models\n",
      "for the detection task, ground-truth labels that truly reflect the nature of outliers in a domain is\n",
      "essential for the reliable error estimation of various models. Thereby, the scarcity of representative\n",
      "Authors‚Äô addresses: Lingxiao Zhao, lingxia1@andrew.cmu.edu, Heinz College - Information Systems & Public Policy,\n",
      "Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA, USA, 15213; Leman Akoglu, lakoglu@andrew.cmu.edu, Heinz\n",
      "College - Information Systems & Public Policy, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA, USA, 15213.\n"
     ]
    }
   ],
   "source": [
    "print(pdf_data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28194272-e429-48c9-89b6-1c39c02fccf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3931"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eeab98-265f-41ca-8df8-5747399b3eeb",
   "metadata": {},
   "source": [
    "**CharacterTextSplitter**:\n",
    "* Tries to preserve paragraphs, sentences, and words as coherent units.\n",
    "* Can specify chunk_size, chunk_overlap, and separator.\n",
    "* Does not automatically handle very large chunks; instead, it relies on the user setting appropriate values for chunk_size and chunk_overlap.\n",
    "\n",
    "**RecursiveCharacterTextSplitter**:\n",
    "* Similar to CharacterTextSplitter, but adds recursive splitting capabilities.\n",
    "* Automatically handles very large chunks by attempting to split them according to the specified chunk_size and separator list.\n",
    "* If a chunk remains too large after the first round of splitting, it will try again with subsequent separators in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f57ec20b-ac28-4b66-a6df-1b03840077d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split [\"\\n\\n\", \"\\n\", \" \", \"\" ]\n",
    "splitterR = RecursiveCharacterTextSplitter(\n",
    "     chunk_size = 1000,\n",
    "     chunk_overlap = 0\n",
    ")\n",
    "\n",
    "pdf_data_RS = pdf_loader.load_and_split(text_splitter=splitterR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39cc0514-b731-4f3f-b376-e7d7a824b91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_data_RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0dc0cfcc-9b8a-4bc6-94fb-9c5af4f50359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Using Classification Datasets to Evaluate Graph Outlier\n",
      "Detection: Peculiar Observations and New Insights\n",
      "LINGXIAO ZHAO, Heinz College - Information Systems & Public Policy, Carnegie Mellon University, USA\n",
      "LEMAN AKOGLU, Heinz College - Information Systems & Public Policy, Carnegie Mellon University, USA\n",
      "It is common practice of the outlier mining community to repurpose classification datasets toward evaluating\n",
      "various detection models. To that end, often a binary classification dataset is used, where samples from\n",
      "one of the classes is designated as the ‚Äòinlier‚Äô samples, and the other class is substantially down-sampled\n",
      "to create the (ground-truth) ‚Äòoutlier‚Äô samples. Graph-level outlier detection (GLOD) is rarely studied but\n",
      "has many potentially influential real-world applications. In this study, we identify an intriguing issue with\n",
      "repurposing graph classification datasets for GLOD . We find that ROC-AUC performance of the models\n"
     ]
    }
   ],
   "source": [
    "print(pdf_data_RS[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36758b88-acc0-4638-8f72-2745e59d3bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_data_RS[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d84c8ee-6e67-47c9-a707-ef9ee36cc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.llms.llamafile import Llamafile\n",
    "\n",
    "# Initialize Llamafile\n",
    "llm = Llamafile(temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c8991aea-5876-4fe9-8f44-6395a3c4e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"tell me a quick joke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cfd66f3e-eacb-42e1-a4c9-6b8821cbf893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "JESSICA: (nodding) \"Who's up for a game of truth or dare?\"\n",
      "\n",
      "LUCAS: (smiling) Sounds fun!\n",
      "\n",
      "CARLA: (smiling) Yeah, I don't know about this.\n",
      "\n",
      "(Jessica shakes her head and the others start laughing.)\n",
      "\n",
      "Narrator (voiceover): As Jessica and Lucas get to know each other better, they find themselves sharing their deepest fears and secrets with one another.\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. LUCAS'S APARTMENT - DAY\n",
      "\n",
      "Lucas is sitting on his couch, staring at his phone, when he gets a call from Jessica.\n",
      "\n",
      "JESSICA: (on the line) Hi, Lucas! I just wanted to say that I appreciate everything you did for me and my family during our ordeal. I know it wasn't easy, but you were always there for us.\n",
      "\n",
      "LUCAS: (smiling) Thanks, Jess. You're welcome. How are you doing?\n",
      "\n",
      "JESSICA: (sighing) I don't know. It feels like a million years since everything was normal.\n",
      "\n",
      "(Lucas reaches out to give her a hug.)\n",
      "\n",
      "LUCAS: (holding her tightly) Don't worry about it, Jess. Things will get better. We have each other.\n",
      "\n",
      "JESSICA: (smiling) Thanks, Lucas. You never know what someone is going through until they experience it with you.\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. JESSICA'S APARTMENT - NIGHT\n",
      "\n",
      "Lucas and Jessica are sitting on the couch, watching a movie together. They have been talking about their lives for hours, but they both know that there is still so much they don't understand.\n",
      "\n",
      "LUCAS: (softly) I wish we could go back to before everything happened. We could start over and figure out how to make things work.\n",
      "\n",
      "JESSICA: (smiling) But it's too late for that now. We have to deal with what we have.\n",
      "\n",
      "(Lucas takes her hand.)\n",
      "\n",
      "LUCAS: (nodding) I know, Jess. And I promise I will do everything in my power to make things right between us.\n",
      "\n",
      "JESSICA: (smiling) Thanks, Lucas. You mean a lot to me.\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. LUCAS'S APARTMENT - NIGHT\n",
      "\n",
      "Lucas and Jessica are sitting on the couch, watching the same movie again, but now they have become more comfortable with each other. They start sharing intimate details of their lives that they haven't shared with anyone else before.\n",
      "\n",
      "JESSICA: (smiling) I still remember that night you picked me up in my car. You were so protective and brave.\n",
      "\n",
      "LUCAS: (laughing softly) Yeah, and it was the first time I felt like a real man.\n",
      "\n",
      "(They both laugh.)\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. LUCAS'S APARTMENT - DAY\n",
      "\n",
      "Lucas is sitting on his couch, playing with Jessica's dog, Max. They are snuggling and watching TV when Lucas notices a fly buzzing around the room.\n",
      "\n",
      "JESSICA: (examining her ear) Did you hear that?\n",
      "\n",
      "LUCAS: (looking up from the TV screen) Yeah, I heard it too. It's buzzing around my head.\n",
      "\n",
      "(Lucas moves his hand closer to Jessica's ear.)\n",
      "\n",
      "JESSICA: (nodding) That sounded like a fly. What is it?\n",
      "\n",
      "LUCAS: (smiling) I think we have a little bug problem in our house. It seems to be hanging around the edges of everything we touch.\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. LUCAS'S APARTMENT - DAY\n",
      "\n",
      "Lucas and Jessica are sitting on the couch, watching TV. They start talking about their favorite shows again, but this time they have been discussing their fears and insecurities.\n",
      "\n",
      "JESSICA: (smiling) I always thought I was a bit too pretty to be a successful actress. Now I know that's not true.\n",
      "\n",
      "LUCAS: (grinning) Me too, Jess. But we can work on that.\n",
      "\n",
      "(They both laugh.)\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. LUCAS'S BEDROOM - NIGHT\n",
      "\n",
      "Lucas is lying in bed, looking up at the ceiling as he thinks about Jessica. He feels a deep sense of longing for her, but at the same time, he knows that their relationship is not meant to be a romantic one.\n",
      "\n",
      "JESSICA: (entering his room) Lucas?\n",
      "\n",
      "LUCAS: (leaning back in bed) Yeah, what's up?\n",
      "\n",
      "JESSICA: (smiling) I was just thinking that we have such a unique relationship. It feels like there's something special between us.\n",
      "\n",
      "LUCAS: (smiling back) I know, and it's scary how much I love you. But at the same time, I also know how much I don't want to hurt you in any way.\n",
      "\n",
      "JESSICA: (nodding) I understand that. But I just wanted to say that I love you too. And I think we're meant to be together.\n",
      "\n",
      "LUCAS: (smiling) Me too, Jess. I'll do everything I can to make things work between us.\n",
      "\n",
      "JESSICA: (hugging him tightly) I love you too, Lucas.\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. JESSICA'S APARTMENT - DAY\n",
      "\n",
      "Lucas and Jessica are sitting on the couch, talking about their dreams for the future. They both know that they want to make things work between them and start a family someday.\n",
      "\n",
      "JESSICA: (nodding) I always wanted kids. Can we adopt?\n",
      "\n",
      "LUCAS: (smiling) That's a great idea. And we can name the baby after you, Jessica.\n",
      "\n",
      "JESSICA: (laughing) Okay, but please don't tell anyone that I am pregnant. I need to keep my secret safe.\n",
      "\n",
      "LUCAS: (smiling) No problem. But I promise I will protect your privacy at all costs.\n",
      "\n",
      "JESSICA: (nodding) That's what friends are for, Lucas. Thank you.\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. JESSICA'S APARTMENT - DAY\n",
      "\n",
      "Lucas and Jessica are sitting on the couch, looking at pictures of their family. They both know that they want to start a family sooner rather than later.\n",
      "\n",
      "JESSICA: (holding up a picture) Look how big this baby is! I can't believe we have another life waiting for us.\n",
      "\n",
      "LUCAS: (smiling) Yeah, and the best part of all? We are going to be able to spend every moment with our child.\n",
      "\n",
      "JESSICA: (nodding) Exactly. And as long as we make it work between us, everything is possible.\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. LUCAS'S BEDROOM - NIGHT\n",
      "\n",
      "Lucas and Jessica are lying in bed, looking up at the ceiling. They both feel a sense of calm and contentment wash over them.\n",
      "\n",
      "JESSICA: (smiling) Lucas, you know what I've been thinking?\n",
      "\n",
      "LUCAS: (entering the room) Yeah, what is it?\n",
      "\n",
      "JESSICA: (leaning back in bed) I have been thinking that we have so much love between us, and it makes me feel good.\n",
      "\n",
      "LUCAS: (smiling back) And you're right. Love is a powerful force that can bring people together even if they are miles apart.\n",
      "\n",
      "JESSICA: (nodding) I agree. And we have each other, Lucas. We've been through so much together, and it makes our love stronger.\n",
      "\n",
      "LUCAS: (smiling) That's true. And I promise I will always be there for you no matter what happens in the future.\n",
      "\n",
      "JESSICA: (smiling back) Same here, Lucas. Thank you for being a part of my life.\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. LUCAS'S BEDROOM - NIGHT\n",
      "\n",
      "Lucas and Jessica are lying in bed, holding each other tightly. They both feel content and happy.\n",
      "\n",
      "JESSICA: (sighing) Lucas, I just wanted to say that even though we have different dreams for the future, I always knew that you were my true love.\n",
      "\n",
      "LUCAS: (smiling back) Same here, Jess. And I know that in the end, all our differences will only make us stronger as a couple.\n",
      "\n",
      "JESSICA: (nodding) That's right. And we can face any challenge together. We have each other, Lucas.\n",
      "\n",
      "LUCAS: (grinning) Absolutely, Jessica. Let's get ready for tomorrow, and the adventures that lie ahead!</s>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f0cd9-b0df-4c6d-8d97-f272b26fae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = Llamafile(temperature=0)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(chain.invoke(pdf_data[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f08a5-9b2e-4410-b22e-fb3bcf0eaa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "Write a summary that highlights the main ideas as bullet points of the following:\n",
    "\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "\n",
    "SUMMARY IN BULLETS:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "print(chain.run(pdf_data[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c2811-2178-4b3c-beda-ed721a1376c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_openai(user_input):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ],\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def start_chatbot():\n",
    "    print(\"üëã Welcome I'm your chatbot. Type 'exit' to end the chat.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye üëã\")\n",
    "            break\n",
    "        response = chat_with_openai(user_input)\n",
    "        print(f\"Bot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8fad82-d569-409b-afbc-0776ae0e07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llms]",
   "language": "python",
   "name": "conda-env-llms-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
